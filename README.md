# orion
A research repo for long-context, decoder-only Transformers using structured sparse attention (sliding window + expander links) and stability controls (QK-norm, orthogonal init, spectral normalization). Includes reproducible training/eval configs and benchmarks across 512â€“4K context lengths for quality, throughput, VRAM, and training stability
